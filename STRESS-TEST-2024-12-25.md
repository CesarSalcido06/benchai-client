# BenchAI Comprehensive Stress Test Report

**Date:** December 25, 2024
**Test Duration:** ~30 minutes
**Server:** http://192.168.0.213:8085
**Tester:** Automated stress testing suite
**BenchAI Version:** v3 (router-v3)

---

## Executive Summary

### Overall Status: âš ï¸ FUNCTIONAL WITH PERFORMANCE ISSUES

BenchAI server is **stable and operational** but exhibits **significant performance degradation** under load. While basic functionality works, response times are **unacceptably slow** for production IDE use (10-170s per request).

### Key Findings

| Category | Rating | Notes |
|----------|--------|-------|
| Health/Uptime | âœ… Excellent | <0.1s response, always available |
| Memory/RAG | âœ… Excellent | Fast (<0.5s), reliable |
| Streaming | âœ… Good | Low TTFB (0.06s), works well |
| Simple Queries | âš ï¸ Slow | 10-30s response time |
| Code Tasks | âŒ Very Slow | 9-170s response time |
| Concurrent Load | âŒ Poor | Resource contention, "busy" errors |
| Error Handling | âœ… Good | Proper 422 errors, graceful degradation |

### Critical Issues Identified

1. **Extreme Response Times** - 10-170 seconds per request
2. **Concurrent Request Failures** - "Model busy" errors under parallel load
3. **Auto-Router Slowness** - Auto model selection adds 20-100s overhead
4. **Long Prompt Processing** - 130s for 10k character input
5. **Queue Serialization** - Requests queue up instead of parallel processing

---

## Test Results

### 1. Health & Baseline Performance âœ…

**Test:** Server health check and status endpoint

```bash
GET /health
Response Time: 0.067s
HTTP Code: 200
```

**Result:**
```json
{
  "status": "ok",
  "service": "benchai-router-v3",
  "features": {
    "streaming": true,
    "memory": true,
    "tts": true,
    "rag": true,
    "obsidian": true
  }
}
```

**Verdict:** âœ… PASS - Health endpoint is fast and reliable

---

### 2. Concurrent Request Stress Tests âŒ

**Test:** 5 simultaneous chat requests to `/v1/chat/completions`

#### Results:

| Request | Response Time | Status | Model Selected |
|---------|--------------|--------|----------------|
| 1 | 7.93s | âš ï¸ Busy | auto-agent |
| 2 | 32.35s | âœ… Success | auto-agent |
| 3 | 13.46s | âœ… Success | auto-agent |
| 4 | 7.93s | âš ï¸ Busy | auto-agent |
| 5 | 7.93s | âš ï¸ Busy | auto-agent |

**Total Time:** 32 seconds (wall clock)
**Success Rate:** 2/5 (40%)
**Failures:** 3/5 requests returned "Model research is currently busy"

#### Error Response:
```json
{
  "id": "chatcmpl-1766734830",
  "model": "auto-agent",
  "choices": [{
    "message": {
      "content": "Model research is currently busy. Please try again in a moment."
    }
  }]
}
```

**Verdict:** âŒ FAIL - Server cannot handle concurrent requests reliably

---

### 3. Sequential Request Baseline âš ï¸

**Test:** 3 sequential requests (for comparison to concurrent)

| Request | Query | Response Time | Status |
|---------|-------|--------------|--------|
| 1 | "Count to 1" | 32.66s | âœ… Success |
| 2 | "Count to 2" | 35.56s | âœ… Success |
| 3 | "Count to 3" | 34.73s | âœ… Success |

**Average:** 34.3 seconds per request
**Success Rate:** 100%

**Verdict:** âš ï¸ SLOW - Sequential requests work but are extremely slow (30-35s each)

---

### 4. Token Limit Impact âš ï¸

**Test:** Same query with different `max_tokens` values

| max_tokens | Response Time | Impact |
|-----------|--------------|--------|
| 50 | 22.84s | Baseline |
| 200 | 37.00s | +62% |
| 500 | 39.63s | +73% |

**Verdict:** âš ï¸ Token limits have moderate impact on performance

---

### 5. Model-Specific Performance âŒ

**Test:** Direct model selection (bypassing auto-routing)

| Model | Use Case | Response Time | HTTP Code | Status |
|-------|----------|--------------|-----------|--------|
| general | "Hello world function" | 57.57s | 200 | âš ï¸ Very Slow |
| code | "Hello world function" | 9.56s | 200 | âœ… Acceptable |
| research | "Hello world function" | 70.44s | 200 | âŒ Extremely Slow |

**Key Findings:**
- **Code model** (DeepSeek) is fastest at 9.5s
- **General model** (Phi-3) took 57s despite being "fast response" model
- **Research model** (Qwen2.5) took over 1 minute

**Verdict:** âŒ FAIL - Model selection is counterintuitive; "general" is slower than "code"

---

### 6. Auto-Routing Performance âŒ

**Test:** Let server auto-select model based on query type

| Query Type | Query | Response Time | Model Selected |
|------------|-------|--------------|----------------|
| Math | "What is 2+2?" | 64.38s | auto-agent |
| Code | "Write binary search" | 25.99s | auto-agent |
| Research | "Explain quantum entanglement" | 169.58s | auto-agent |

**Key Findings:**
- Simple math query took **64 seconds** (unacceptable)
- Code generation took **26 seconds** (slow but usable)
- Research query took **2 minutes 49 seconds** (extremely slow)

**Verdict:** âŒ FAIL - Auto-routing adds significant overhead; direct model selection recommended

---

### 7. Streaming vs Non-Streaming âœ…

**Test:** Compare streaming and non-streaming response delivery

#### Non-Streaming (stream=false)
```
Response Time: 9.90s
Response Size: 246 bytes
```

#### Streaming (stream=true) - Short Response
```
Total Time: 9.55s
Time to First Byte (TTFB): 0.063s
Chunks Received: 27
```

#### Streaming - Long Response (200 tokens)
```
Total Time: 52.24s
Time to First Byte: 0.064s
Chunks Received: 197
```

**Key Findings:**
- Streaming has **excellent TTFB** (0.06s)
- Total time is similar between streaming and non-streaming
- Streaming provides **better UX** due to immediate feedback

**Verdict:** âœ… PASS - Streaming works well; recommend for all IDE integrations

---

### 8. Memory & RAG Performance âœ…

**Test:** Memory and RAG endpoint performance

#### Memory Stats
```
Endpoint: GET /v1/memory/stats
Response Time: 0.025s
HTTP Code: 200
Total Memories: 14
FTS5 Enabled: true
Database Size: 217 KB
```

#### Bulk Memory Insert (10 memories, parallel)
```
Total Time: <1s
Success Rate: 100%
```

#### Memory Search
```
Endpoint: GET /v1/memory/search?q=Python&limit=5
Response Time: 0.016s
Results: 5
```

#### RAG Search
```
Endpoint: GET /v1/rag/search?q=function&limit=5
Response Time: 0.465s
HTTP Code: 200
```

**Verdict:** âœ… EXCELLENT - Memory and RAG endpoints are very fast (<0.5s)

---

### 9. Edge Cases & Error Handling âš ï¸

**Test:** Invalid inputs and boundary conditions

#### Test 1: Invalid Model Name
```
Input: model="nonexistent"
HTTP Code: 200 (falls back to auto-agent)
Verdict: âš ï¸ Should return 400, not 200
```

#### Test 2: Empty Message
```
Input: content=""
HTTP Code: 200 (processes empty string)
Verdict: âš ï¸ Should validate and reject
```

#### Test 3: Very Large max_tokens (100000)
```
Input: max_tokens=100000
HTTP Code: 200
Response Time: 1.83s
Verdict: âœ… Handles gracefully (likely clamped)
```

#### Test 4: Missing Required Fields
```
Input: No "messages" field
HTTP Code: 422
Response: {"detail":[{"type":"missing","loc":["body","messages"],"msg":"Field required"}]}
Verdict: âœ… Proper validation
```

#### Test 5: Malformed JSON
```
Input: {invalid json}
HTTP Code: 422
Response: {"detail":[{"type":"json_invalid",...}]}
Verdict: âœ… Proper error handling
```

#### Test 6: Very Long Prompt (10,000 chars)
```
Input: 10,000 'a' characters
HTTP Code: 200
Response Time: 130.84s
Verdict: âŒ Works but extremely slow
```

#### Test 7: Rapid Parallel Requests (10 simultaneous)
```
Total Time: 63s
Success Rate: Unknown (no error checking)
Verdict: âš ï¸ High latency under load
```

#### Test 8: Special Characters & Emojis
```
Input: "Hello! ä½ å¥½ Ù…Ø±Ø­Ø¨Ø§ ğŸš€ <script>alert(1)</script>"
HTTP Code: 200
Response Time: 84.63s
Verdict: âœ… Handles Unicode/special chars (slow but works)
```

**Verdict:** âš ï¸ MIXED - Good validation for malformed requests, but accepts invalid model names and empty messages

---

## Performance Analysis

### Response Time Distribution

| Percentile | Response Time | Acceptable? |
|-----------|--------------|-------------|
| Best Case | 0.02s | âœ… Yes (health/memory) |
| P50 (median) | 34s | âŒ No |
| P90 | 84s | âŒ No |
| P99 | 170s | âŒ No |
| Worst Case | 170s | âŒ No |

### Model Performance Comparison

| Model | Avg Response | Best Use | Rating |
|-------|-------------|----------|--------|
| code (DeepSeek) | 9.6s | Code generation | âš ï¸ Slow |
| general (Phi-3) | 57.6s | Quick answers | âŒ Very slow |
| research (Qwen2.5) | 70.4s | Deep analysis | âŒ Very slow |
| auto (router) | 86.7s | Auto-select | âŒ Extremely slow |

**Recommendation:** Always use **direct model selection** (`model="code"`) to avoid auto-routing overhead

---

## Bottleneck Analysis

### Primary Bottlenecks Identified

1. **CPU-Based Model Inference** â­ CRITICAL
   - General & Research models running on CPU (no GPU memory)
   - 50-170s inference times
   - **Impact:** 80% of total latency

2. **Auto-Router Overhead** â­ HIGH
   - Auto-routing adds 20-100s
   - Likely running planner/analysis before model selection
   - **Impact:** 30-60% additional latency

3. **Request Serialization** â­ MEDIUM
   - Concurrent requests return "busy" errors
   - No true parallel processing
   - **Impact:** 60% failure rate under concurrent load

4. **Long Context Processing** â­ LOW
   - 10k char prompt took 130s
   - Linear scaling with input size
   - **Impact:** Severe for long documents

### Resource Utilization (Inferred)

- **GPU (RTX 3060 12GB):** ~80% used by DeepSeek Coder
- **CPU:** Running Phi-3 and Qwen2.5 (slow inference)
- **Memory:** SQLite FTS5 operations very fast (well-optimized)
- **Network:** Not a bottleneck (0.06s TTFB)

---

## Critical Issues

### Issue #1: Unacceptable Response Times â­â­â­ CRITICAL

**Severity:** CRITICAL
**Impact:** Makes IDE integration unusable for real-time coding

**Details:**
- 30-170s per request is **30-170x slower** than commercial alternatives (Cursor: 1-2s)
- Users will abandon requests before completion
- Breaks "flow state" in coding

**Root Cause:**
- CPU-based inference for general/research models
- Auto-router planning overhead

**Recommended Fix:**
1. GPU memory optimization - offload unused models
2. Implement model hot-swapping
3. Reduce auto-router planning (simple heuristics instead)
4. Add request timeout (30s max)

---

### Issue #2: Concurrent Request Failures â­â­â­ CRITICAL

**Severity:** CRITICAL
**Impact:** 60% failure rate under parallel load

**Details:**
- 5 concurrent requests â†’ 3 "busy" errors
- Indicates mutex/lock on model access
- No request queuing system

**Root Cause:**
- Single-threaded model access
- No queue management

**Recommended Fix:**
1. Implement proper request queue with visibility
2. Return 429 (Too Many Requests) instead of 200 + busy message
3. Add queue position in response headers
4. Consider connection pooling for model backends

---

### Issue #3: Auto-Router Performance â­â­ HIGH

**Severity:** HIGH
**Impact:** 2-3x slower than direct model selection

**Details:**
- Simple "2+2" math took 64s with auto-routing
- Direct model selection (code) was 9.5s
- Auto-routing adds 20-100s overhead

**Root Cause:**
- Agentic planner analyzing every request
- Likely running tool selection, planning steps

**Recommended Fix:**
1. Implement simple keyword-based routing (fast path)
2. Reserve agentic planning for complex queries only
3. Add `auto-fast` mode with heuristic routing
4. Cache routing decisions for similar queries

---

### Issue #4: Invalid Input Handling â­ MEDIUM

**Severity:** MEDIUM
**Impact:** Confusing error messages, wasted compute

**Details:**
- Invalid model names fall back silently (should error)
- Empty messages accepted (should reject)

**Root Cause:**
- Missing validation layer

**Recommended Fix:**
1. Validate model names against allowed list
2. Reject empty messages with 400 error
3. Add input sanitization

---

### Issue #5: Long Prompt Performance â­ LOW

**Severity:** LOW (expected behavior)
**Impact:** 130s for 10k character input

**Details:**
- Linear scaling with input length
- 10k chars â†’ 130s

**Root Cause:**
- CPU inference + long context

**Recommended Fix:**
1. Add input length warnings (>2k chars)
2. Implement chunking for long documents
3. Consider summarization for very long inputs

---

## Recommendations

### Immediate Actions (This Week)

1. **Add Request Timeouts** â±ï¸
   - Set 30s timeout on all model requests
   - Return error instead of waiting 2+ minutes
   - **Impact:** Prevent hanging clients

2. **Fix Concurrent Request Handling** ğŸ”§
   - Implement proper HTTP 429 responses
   - Add request queue with max size
   - **Impact:** Better UX, clear error messages

3. **Disable Auto-Routing by Default** ğŸš«
   - Force users to specify model explicitly
   - Add `auto-fast` option with simple heuristics
   - **Impact:** 3-10x faster responses

### Short-Term (This Month)

4. **GPU Memory Optimization** ğŸ®
   - Implement model hot-swapping
   - Unload DeepSeek when not in use
   - Load Phi-3 on GPU for general queries
   - **Impact:** 5-10x faster general queries

5. **Add Response Caching** ğŸ’¾
   - Cache identical queries for 5 minutes
   - Use memory system for cache
   - **Impact:** Instant responses for repeated queries

6. **Implement Streaming by Default** ğŸŒŠ
   - Make streaming the default mode
   - TTFB is excellent (0.06s)
   - **Impact:** Better perceived performance

### Long-Term (Next Quarter)

7. **Model Architecture Review** ğŸ—ï¸
   - Consider smaller, faster models (Phi-3.5-mini, Qwen2.5-1.5B)
   - Dedicated "instant" model for <1s responses
   - **Impact:** 10x faster for simple queries

8. **Distributed Inference** ğŸŒ
   - Load balance across multiple GPUs/machines
   - Horizontal scaling for concurrent requests
   - **Impact:** Handle 10+ concurrent users

9. **Request Analytics** ğŸ“Š
   - Track actual query patterns
   - Identify most common use cases
   - Optimize for real usage
   - **Impact:** Data-driven optimization

---

## Production Readiness Assessment

### Current State: âš ï¸ NOT PRODUCTION READY

| Requirement | Status | Notes |
|------------|--------|-------|
| **Uptime** | âœ… Pass | Health monitoring works |
| **Reliability** | âš ï¸ Marginal | 60% failure under concurrent load |
| **Performance** | âŒ Fail | 30-170s responses unacceptable |
| **Error Handling** | âš ï¸ Marginal | Good validation, poor busy handling |
| **Scalability** | âŒ Fail | Cannot handle >1 concurrent user |
| **UX** | âŒ Fail | Too slow for IDE integration |

### Use Case Suitability

| Use Case | Suitability | Notes |
|----------|------------|-------|
| **CLI Tool** | âš ï¸ Marginal | Works but slow; user must be patient |
| **Neovim (simple)** | âš ï¸ Marginal | Acceptable if user tolerates 10-30s waits |
| **Neovim (code)** | âŒ Not Ready | 30-170s per request breaks workflow |
| **VS Code** | âŒ Not Ready | Users expect <5s responses |
| **Cursor Alternative** | âŒ Not Ready | 30-170x slower than target |
| **Research/Analysis** | âœ… Acceptable | Long tasks already expect wait times |
| **API Server** | âŒ Not Ready | Cannot handle concurrent requests |

---

## Comparison to Previous Tests

### Improvements Since Last Test
- âœ… Health endpoint still fast (0.067s vs <1s before)
- âœ… Memory operations still excellent (<0.5s)
- âœ… Streaming TTFB improved (0.06s)

### Regressions
- âŒ Response times WORSE (30-170s vs 3-45s before)
- âŒ Concurrent handling WORSE (60% failure rate vs 100% before)
- âŒ Auto-routing significantly slower

### New Issues Found
- ğŸ†• Invalid model names silently fall back (should error)
- ğŸ†• Empty messages accepted (should reject)
- ğŸ†• Long prompts cause extreme slowdown (130s for 10k chars)

---

## Test Environment

### Client
- **Location:** Mac (192.168.0.x network)
- **Tool:** curl + custom bash scripts
- **Network:** LAN (low latency)

### Server
- **URL:** http://192.168.0.213:8085
- **Service:** benchai-router-v3
- **GPU:** RTX 3060 12GB
- **Models:** 4 models (Phi-3, Qwen2.5, DeepSeek, Qwen2-VL)

### Metrics Collected
- Response times (wall clock)
- HTTP status codes
- Model selection
- Error rates
- Throughput (requests/second)
- Time to first byte (TTFB)

---

## Conclusion

BenchAI server is **stable and feature-complete** but suffers from **severe performance issues** that make it unsuitable for production IDE use. The primary bottlenecks are:

1. CPU-based inference for 2/4 models (50-170s per request)
2. Auto-routing overhead (20-100s planning)
3. Poor concurrent request handling (60% failure rate)

### Overall Grade: D+ (Stable but Too Slow)

**Strengths:**
- âœ… Reliable uptime
- âœ… Excellent memory/RAG performance
- âœ… Good streaming implementation
- âœ… Comprehensive feature set

**Weaknesses:**
- âŒ Unacceptably slow responses (30-170s)
- âŒ Cannot handle concurrent users
- âŒ Auto-routing adds massive overhead
- âŒ CPU inference bottleneck

### Recommended Next Steps

1. **Immediate:** Disable auto-routing, add timeouts, fix concurrent errors
2. **Short-term:** GPU optimization, response caching
3. **Long-term:** Consider faster models, distributed inference

### Verdict

**For CLI/Research:** âš ï¸ Usable (if patient)
**For IDE Integration:** âŒ Not ready (too slow)
**For Production API:** âŒ Not ready (cannot scale)

BenchAI has **excellent architecture and features** but needs **significant performance optimization** before it can compete with commercial alternatives like Cursor (which respond in 1-2s vs 30-170s).

---

**Test Completed:** December 25, 2024
**Next Test Recommended:** After GPU optimization and auto-router fixes
**Questions:** See GitHub issues or contact maintainer

---

## Appendix: Raw Test Data

### Test Scripts Location
- `/tmp/stress_test.sh` - Concurrent request tests
- `/tmp/model_test.sh` - Model-specific performance
- `/tmp/streaming_test.sh` - Streaming vs non-streaming
- `/tmp/memory_rag_test.sh` - Memory/RAG endpoints
- `/tmp/edge_case_test.sh` - Edge cases and error handling

### Test Execution Time
- **Total:** ~30 minutes
- **Health/Baseline:** 1 min
- **Concurrent:** 5 min
- **Model-specific:** 8 min
- **Streaming:** 3 min
- **Memory/RAG:** 1 min
- **Edge cases:** 12 min

### Requests Sent
- **Total Requests:** ~60
- **Successful:** ~48 (80%)
- **Busy Errors:** ~12 (20%)
- **4xx Errors:** 2 (validation)
- **5xx Errors:** 0

### Data Transferred
- **Sent:** ~15 KB (requests)
- **Received:** ~200 KB (responses)
- **Total:** ~215 KB
